"""
ë¡œê³  í¬ë¡¤ë§ ëª¨ë“ˆ
TradingViewì™€ logo.devì—ì„œ ë¡œê³ ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê¸°ëŠ¥
"""

import asyncio
import aiohttp
import requests
import hashlib
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from io import BytesIO
import json

from playwright.async_api import async_playwright
from fake_useragent import UserAgent
from PIL import Image
from minio import Minio

class DateTimeEncoder(json.JSONEncoder):
    """datetime ê°ì²´ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”í•˜ëŠ” ì»¤ìŠ¤í…€ ì¸ì½”ë”"""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)

class LogoCrawler:
    """ë¡œê³  í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.minio_client = Minio(
            os.getenv('MINIO_ENDPOINT', 'minio:9000'),
            access_key=os.getenv('MINIO_ACCESS_KEY', 'minioadmin'),
            secret_key=os.getenv('MINIO_SECRET_KEY', 'minioadmin123'),
            secure=False
        )
        self.bucket = os.getenv('MINIO_BUCKET', 'logos')
        self.existing_api_base = os.getenv('EXISTING_API_BASE', 'http://10.150.2.150:8004')
        self.logo_dev_token = os.getenv('LOGO_DEV_TOKEN')
        
    async def crawl_tradingview(self, infomax_code: str, ticker: str) -> Optional[bytes]:
        """TradingViewì—ì„œ ë¡œê³  í¬ë¡¤ë§"""
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent=self.ua.random
                )
                # í˜ì´ì§€ íƒ€ì„ì•„ì›ƒ ì„¤ì •
                context.set_default_timeout(20000)  # 20ì´ˆ
                
                page = await context.new_page()
                
                # TradingView í˜ì´ì§€ë¡œ ì´ë™
                url = f"https://www.tradingview.com/symbols/{ticker}/"
                await page.goto(url, timeout=20000)
                
                # ë¡œê³  ì´ë¯¸ì§€ ì„ íƒì (ì—¬ëŸ¬ ê°€ëŠ¥ì„± ì‹œë„)
                selectors = [
                    'img[data-testid="logo"]',
                    '.tv-symbol-header__logo img',
                    '.tv-symbol-header__logo svg',
                    'img[alt*="logo" i]',
                    'img[src*="logo" i]',
                    '.tv-symbol-header img',
                    'header img'
                ]
                
                for selector in selectors:
                    try:
                        element = await page.wait_for_selector(selector, timeout=15000)
                        if element:
                            # SVGì¸ ê²½ìš°
                            if 'svg' in selector:
                                svg_content = await element.inner_html()
                                return svg_content.encode('utf-8')
                            # IMGì¸ ê²½ìš°
                            else:
                                src = await element.get_attribute('src')
                                if src and src.startswith('http'):
                                    timeout = aiohttp.ClientTimeout(total=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                                    async with aiohttp.ClientSession(timeout=timeout) as session:
                                        async with session.get(src) as response:
                                            if response.status == 200:
                                                return await response.read()
                    except:
                        continue
                
                await browser.close()
                return None
                
        except Exception as e:
            print(f"TradingView í¬ë¡¤ë§ ì˜¤ë¥˜ ({infomax_code}): {e}")
            return None
    
    async def crawl_logo_dev(self, infomax_code: str, api_domain: str) -> Optional[bytes]:
        """logo.dev APIì—ì„œ ë¡œê³  í¬ë¡¤ë§"""
        try:
            if not self.logo_dev_token:
                print("LOGO_DEV_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return None
            
            # API ì¿¼í„° í™•ì¸
            if not await self._check_quota('logo_dev'):
                print("logo.dev ì¼ì¼ ì¿¼í„° ì´ˆê³¼")
                return None
            
            url = f"https://img.logo.dev/{api_domain}?token={self.logo_dev_token}&format=png&size=300&fallback=404"
            
            timeout = aiohttp.ClientTimeout(total=15)  # 15ì´ˆ íƒ€ì„ì•„ì›ƒ
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        # ì¿¼í„° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
                        await self._update_quota('logo_dev')
                        return await response.read()
                    else:
                        print(f"logo.dev API ì˜¤ë¥˜: {response.status}")
                        return None
                        
        except Exception as e:
            print(f"logo.dev í¬ë¡¤ë§ ì˜¤ë¥˜ ({infomax_code}): {e}")
            return None
    
    async def _check_quota(self, provider: str) -> bool:
        """API ì¿¼í„° í™•ì¸"""
        try:
            # ê¸°ì¡´ APIë¥¼ í†µí•´ ì¿¼í„° í™•ì¸
            timeout = aiohttp.ClientTimeout(total=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
            async with aiohttp.ClientSession(timeout=timeout) as session:
                url = f"{self.existing_api_base}/api/query/raw_data/ext_api_quota"
                params = {
                    "api_name": provider,
                    "quota_date": datetime.now().strftime('%Y-%m-%d'),
                    "limit": 1
                }
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        print(f"ğŸ” ì¿¼í„° í™•ì¸ ì‘ë‹µ: {data}")
                        # í˜ì´ì§€ë„¤ì´ì…˜ëœ ì‘ë‹µ êµ¬ì¡° í™•ì¸
                        if 'data' in data and data['data'] and len(data['data']) > 0:
                            used_count = data['data'][0].get('used_count', 0)
                            max_count = data['data'][0].get('max_count', 5000)
                            print(f"ğŸ” ì¿¼í„° ì‚¬ìš©ëŸ‰: {used_count}/{max_count}")
                            return used_count < max_count
                        elif data and len(data) > 0:
                            used_count = data[0].get('used_count', 0)
                            max_count = data[0].get('max_count', 5000)
                            print(f"ğŸ” ì¿¼í„° ì‚¬ìš©ëŸ‰ (ì§ì ‘): {used_count}/{max_count}")
                            return used_count < max_count
            print(f"ğŸ” ì¿¼í„° í™•ì¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ True ë°˜í™˜")
            return True
        except:
            return True
    
    async def _update_quota(self, provider: str):
        """API ì¿¼í„° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸"""
        try:
            # ê¸°ì¡´ APIë¥¼ í†µí•´ ì¿¼í„° ì—…ë°ì´íŠ¸
            timeout = aiohttp.ClientTimeout(total=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
            async with aiohttp.ClientSession(timeout=timeout) as session:
                url = f"{self.existing_api_base}/api/schemas/raw_data/tables/ext_api_quota/upsert"
                data = {
                    "api_name": provider,
                    "quota_date": datetime.now().strftime('%Y-%m-%d'),
                    "used_count": 1,
                    "max_count": 5000
                }
                async with session.post(url, json=data) as response:
                    pass  # ê²°ê³¼ëŠ” ë¬´ì‹œ
        except:
            pass
    
    def convert_image(self, image_data: bytes, infomax_code: str) -> Dict[str, bytes]:
        """ì´ë¯¸ì§€ë¥¼ ë‹¤ì–‘í•œ í¬ê¸°ë¡œ ë³€í™˜"""
        try:
            # SVGì¸ ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if image_data.startswith(b'<svg') or image_data.startswith(b'<?xml'):
                return {"original": image_data}
            
            # BytesIO ê°ì²´ë¥¼ ìƒì„±í•˜ê³  ì´ë¯¸ì§€ ì—´ê¸°
            image_buffer = BytesIO(image_data)
            image = Image.open(image_buffer)
            
            # í‘œì¤€ ì‚¬ì´ì¦ˆë¡œ ë³€í™˜ (í™˜ê²½ë³€ìˆ˜ IMAGE_SIZES ì‚¬ìš©, ê¸°ë³¸ 240,300)
            sizes_env = os.getenv('IMAGE_SIZES', '240,300')
            try:
                sizes = [int(s.strip()) for s in sizes_env.split(',') if s.strip()]
            except Exception:
                sizes = [240, 300]
            formats = ['PNG', 'WebP']
            results = {}
            
            for size in sizes:
                for format_type in formats:
                    # ë¦¬ì‚¬ì´ì¦ˆ
                    resized = image.resize((size, size), Image.Resampling.LANCZOS)
                    
                    # ë°”ì´íŠ¸ë¡œ ë³€í™˜
                    output = BytesIO()
                    if format_type == 'PNG':
                        resized.save(output, format='PNG', optimize=True)
                    else:  # WebP
                        resized.save(output, format='WebP', quality=85, optimize=True)
                    
                    results[f"{format_type.lower()}_{size}"] = output.getvalue()
            
            return results
            
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë³€í™˜ ì˜¤ë¥˜ ({infomax_code}): {e}")
            # ë³€í™˜ ì‹¤íŒ¨í•´ë„ ì›ë³¸ì€ ì €ì¥
            result = {"original": image_data}
            print(f"ğŸ” ë³€í™˜ ì‹¤íŒ¨ë¡œ ì›ë³¸ë§Œ ë°˜í™˜: {len(result)}ê°œ")
            return result
    
    async def save_to_minio(self, image_data: bytes, object_key: str, content_type: str = "image/png"):
        """MinIOì— ì´ë¯¸ì§€ ì €ì¥"""
        try:
            self.minio_client.put_object(
                self.bucket,
                object_key,
                BytesIO(image_data),
                len(image_data),
                content_type=content_type
            )
            return True
        except Exception as e:
            print(f"MinIO ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    async def save_to_database(self, infomax_code: str, logo_hash: str, file_info: Dict):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œê³  ì •ë³´ ì €ì¥"""
        print(f"ğŸ” DB ì €ì¥ ì‹œì‘: {infomax_code}, {logo_hash}")
        try:
            # ê¸°ì¡´ APIë¥¼ í†µí•´ ë°ì´í„° ì €ì¥
            timeout = aiohttp.ClientTimeout(total=15)  # 15ì´ˆ íƒ€ì„ì•„ì›ƒ
            async with aiohttp.ClientSession(timeout=timeout) as session:
                # logos í…Œì´ë¸”ì— ì €ì¥
                logo_url = f"{self.existing_api_base}/api/schemas/raw_data/tables/logos/upsert"
                logo_data = {
                    "data": {
                        "logo_hash": logo_hash,
                        "is_deleted": False,
                        "created_at": datetime.now(),
                        "updated_at": datetime.now()
                    },
                    "conflict_columns": ["logo_hash"]
                }
                async with session.post(logo_url, json=logo_data, headers={'Content-Type': 'application/json'}) as response:
                    print(f"ğŸ” logos í…Œì´ë¸” ì €ì¥ ì‘ë‹µ: {response.status}")
                    if response.status == 200:
                        logo_result = await response.json()
                        print(f"ğŸ” logos ì €ì¥ ê²°ê³¼: {logo_result}")
                        logo_id = logo_result.get('data', {}).get('logo_id')
                        
                        if logo_id:
                            print(f"ğŸ” logo_id íšë“: {logo_id}")
                            # logo_files í…Œì´ë¸”ì— ì €ì¥
                            file_url = f"{self.existing_api_base}/api/schemas/raw_data/tables/logo_files/upsert"
                            file_data = {
                                "data": {
                                    "logo_id": logo_id,
                                    "minio_object_key": file_info['object_key'],
                                    "file_format": file_info['format'],
                                    "dimension_width": file_info['dimension_width'],
                                    "dimension_height": file_info['dimension_height'],
                                    "file_size": file_info['file_size'],
                                    "is_original": file_info.get('is_original', False),
                                    "upload_type": "crawled",
                                    "data_source": file_info['data_source'],
                                    "created_at": datetime.now()
                                },
                                "conflict_columns": ["logo_id", "minio_object_key"]
                            }
                            async with session.post(file_url, json=file_data, headers={'Content-Type': 'application/json'}) as file_response:
                                print(f"ğŸ” logo_files í…Œì´ë¸” ì €ì¥ ì‘ë‹µ: {file_response.status}")
                                if file_response.status == 200:
                                    file_result = await file_response.json()
                                    print(f"ğŸ” logo_files ì €ì¥ ê²°ê³¼: {file_result}")
                                return file_response.status == 200
                        else:
                            print(f"âŒ logo_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {logo_result}")
                    else:
                        print(f"âŒ logos í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨: {response.status}")
                        error_text = await response.text()
                        print(f"âŒ ì˜¤ë¥˜ ë‚´ìš©: {error_text}")
            return False
        except Exception as e:
            print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    async def crawl_logo(self, infomax_code: str, ticker: str, api_domain: str = None) -> bool:
        """ë¡œê³  í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸ”ğŸ”ğŸ” CRAWL_LOGO í•¨ìˆ˜ ì§„ì…: {infomax_code}")
        print(f"ğŸ”ğŸ”ğŸ” íŒŒë¼ë¯¸í„°: ticker={ticker}, api_domain={api_domain}")
        try:
            print(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {infomax_code}, ticker={ticker}, api_domain={api_domain}")
            print(f"ğŸ” í•¨ìˆ˜ ì§„ì… í™•ì¸: {infomax_code}")
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì • (60ì´ˆ)
            import asyncio
            try:
                result = await asyncio.wait_for(
                    self._crawl_logo_internal(infomax_code, ticker, api_domain),
                    timeout=60.0
                )
                print(f"ğŸ”ğŸ”ğŸ” CRAWL_LOGO í•¨ìˆ˜ ì™„ë£Œ: {infomax_code}, ê²°ê³¼: {result}")
                return result
            except asyncio.TimeoutError:
                print(f"ğŸ” í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ: {infomax_code}")
                return False
                
        except Exception as e:
            print(f"ğŸ” í¬ë¡¤ë§ í•¨ìˆ˜ ì˜¤ë¥˜: {infomax_code} - {e}")
            import traceback
            print(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return False
    
    async def _crawl_logo_internal(self, infomax_code: str, ticker: str, api_domain: str = None) -> bool:
        """ë¡œê³  í¬ë¡¤ë§ ë‚´ë¶€ í•¨ìˆ˜"""
        try:
            print(f"ğŸ” ë‚´ë¶€ í•¨ìˆ˜ ì§„ì…: {infomax_code}")
            image_data = None
            data_source = None
            logo_hash = None
            
            # TradingViewì—ì„œ í¬ë¡¤ë§ ì‹œë„ (tickerê°€ ìˆì„ ë•Œë§Œ)
            if ticker and ticker.strip():
                print(f"ğŸ” TradingView í¬ë¡¤ë§ ì‹œë„: {infomax_code}")
                try:
                    image_data = await self.crawl_tradingview(infomax_code, ticker)
                    if image_data:
                        data_source = "tradingview"
                        logo_hash = hashlib.md5(f"tradingview_{infomax_code}".encode()).hexdigest()
                        print(f"ğŸ” TradingView í¬ë¡¤ë§ ì„±ê³µ: {infomax_code}")
                    else:
                        print(f"ğŸ” TradingView í¬ë¡¤ë§ ì‹¤íŒ¨: {infomax_code}")
                except Exception as e:
                    print(f"ğŸ” TradingView í¬ë¡¤ë§ ì˜¤ë¥˜: {infomax_code} - {e}")
            
            # TradingView ì‹¤íŒ¨ ì‹œ logo.dev ì‹œë„
            if not image_data and api_domain:
                print(f"ğŸ” logo.dev í¬ë¡¤ë§ ì‹œë„: {infomax_code}")
                try:
                    image_data = await self.crawl_logo_dev(infomax_code, api_domain)
                    if image_data:
                        data_source = "logo_dev"
                        logo_hash = hashlib.md5(f"logo_dev_{infomax_code}".encode()).hexdigest()
                        print(f"ğŸ” logo.dev í¬ë¡¤ë§ ì„±ê³µ: {infomax_code}")
                    else:
                        print(f"ğŸ” logo.dev í¬ë¡¤ë§ ì‹¤íŒ¨: {infomax_code}")
                except Exception as e:
                    print(f"ğŸ” logo.dev í¬ë¡¤ë§ ì˜¤ë¥˜: {infomax_code} - {e}")
            
            if not image_data:
                print(f"âŒ ëª¨ë“  í¬ë¡¤ë§ ì‹œë„ ì‹¤íŒ¨: {infomax_code}")
                return False
            
            # ì´ë¯¸ì§€ ë³€í™˜
            print(f"ğŸ” ì´ë¯¸ì§€ ë³€í™˜ ì‹œì‘: {infomax_code}")
            converted_images = self.convert_image(image_data, infomax_code)
            print(f"ğŸ” ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {infomax_code}")
            
            # MinIOì— ì €ì¥
            saved_files = []
            print(f"ğŸ” ë³€í™˜ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(converted_images)}")
            for format_key, img_data in converted_images.items():
                if format_key == "original":
                    # logo_hash ê¸°ë°˜ ì €ì¥ìœ¼ë¡œ í†µì¼ (ì›ë³¸ SVGë„ logo_hash í‚¤ ì‚¬ìš©ì„ ìœ„í•´ infomax_code í•´ì‹œ)
                    logo_hash = hashlib.md5(f"{infomax_code}".encode()).hexdigest()
                    object_key = f"{logo_hash}_original.svg"
                    content_type = "image/svg+xml"
                    is_original = True
                    format_type = "svg"
                    size = None
                else:
                    format_type, size = format_key.split('_')
                    logo_hash = hashlib.md5(f"{infomax_code}".encode()).hexdigest()
                    object_key = f"{logo_hash}_{size}.{format_type.lower()}"
                    content_type = f"image/{format_type.lower()}"
                    is_original = False
                
                print(f"ğŸ” MinIO ì €ì¥ ì‹œë„: {object_key}")
                if await self.save_to_minio(img_data, object_key, content_type):
                    print(f"âœ… MinIO ì €ì¥ ì„±ê³µ: {object_key}")
                    saved_files.append({
                        'object_key': object_key,
                        'format': format_type.lower(),
                        'dimension_width': int(size) if size else None,
                        'dimension_height': int(size) if size else None,
                        'file_size': len(img_data),
                        'is_original': is_original,
                        'data_source': data_source
                    })
                else:
                    print(f"âŒ MinIO ì €ì¥ ì‹¤íŒ¨: {object_key}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            print(f"ğŸ” saved_files ê°œìˆ˜: {len(saved_files)}")
            if not saved_files:
                print(f"âŒ ì €ì¥í•  íŒŒì¼ì´ ì—†ìŒ: {infomax_code}")
                return False
                
            for file_info in saved_files:
                print(f"ğŸ” íŒŒì¼ ì €ì¥: {file_info}")
                await self.save_to_database(infomax_code, logo_hash, file_info)
            
            print(f"ë¡œê³  í¬ë¡¤ë§ ì„±ê³µ: {infomax_code} ({len(saved_files)}ê°œ íŒŒì¼)")
            return True
            
        except Exception as e:
            print(f"ë¡œê³  í¬ë¡¤ë§ ì˜¤ë¥˜ ({infomax_code}): {e}")
            return False
    
    async def crawl_batch(self, tickers: List[Dict], job_id: str = None) -> str:
        """ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰"""
        if not job_id:
            job_id = f"crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ì§„í–‰ìƒí™© íŒŒì¼ ìƒì„±
        progress_file = Path("progress") / f"{job_id}.json"
        progress_file.parent.mkdir(exist_ok=True)
        
        progress_data = {
            "job_id": job_id,
            "status": "running",
            "total": len(tickers),
            "completed": 0,
            "success": 0,
            "failed": 0,
            "current": "",
            "started_at": datetime.now().isoformat(),
            "errors": []
        }
        
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        
        try:
            for i, ticker_info in enumerate(tickers):
                infomax_code = ticker_info['infomax_code']
                ticker = ticker_info['ticker']
                api_domain = ticker_info.get('api_domain')
                
                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                progress_data['current'] = f"{infomax_code} ({ticker})"
                progress_data['completed'] = i
                
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, ensure_ascii=False, indent=2)
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                success = await self.crawl_logo(infomax_code, ticker, api_domain)
                
                if success:
                    progress_data['success'] += 1
                else:
                    progress_data['failed'] += 1
                    progress_data['errors'].append(f"Failed: {infomax_code}")
            
            # ì™„ë£Œ ì²˜ë¦¬
            progress_data['status'] = "completed"
            progress_data['completed'] = len(tickers)
            progress_data['finished_at'] = datetime.now().isoformat()
            
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
            
            return job_id
            
        except Exception as e:
            progress_data['status'] = "error"
            progress_data['error'] = str(e)
            progress_data['finished_at'] = datetime.now().isoformat()
            
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
            
            raise e
