"""
ë¡œê³  í¬ë¡¤ë§ ëª¨ë“ˆ
ì›¹ì‚¬ì´íŠ¸ì™€ logo.devì—ì„œ ë¡œê³ ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µ
"""

import asyncio
import aiohttp
import requests
import hashlib
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from io import BytesIO
import json

from playwright.async_api import async_playwright
from fake_useragent import UserAgent
from PIL import Image
from minio import Minio
import logging

logger = logging.getLogger(__name__)

class DateTimeEncoder(json.JSONEncoder):
    """datetime ê°ì²´ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”í•˜ëŠ” ì»¤ìŠ¤í…€ ì¸ì½”ë”"""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)

class LogoCrawler:
    """ë¡œê³  í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.minio_client = Minio(
            os.getenv('MINIO_ENDPOINT', 'minio:9000'),
            access_key=os.getenv('MINIO_ACCESS_KEY', 'minioadmin'),
            secret_key=os.getenv('MINIO_SECRET_KEY', 'minioadmin123'),
            secure=False
        )
        self.bucket = os.getenv('MINIO_BUCKET', 'logos')
        self.existing_api_base = os.getenv('EXISTING_API_BASE', 'http://10.150.2.150:8004')
        self.logo_dev_token = os.getenv('LOGO_DEV_TOKEN')
        
        # existing_api ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        from api_server import existing_api
        self.existing_api = existing_api
        
    async def crawl_website(self, infomax_code: str, ticker: str) -> Optional[bytes]:
        """ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë¡œê³  í¬ë¡¤ë§ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        max_retries = 3
        base_timeout = 10000  # 10ì´ˆ
        
        for attempt in range(max_retries):
            try:
                logger.info(f"ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œë„ {attempt + 1}/{max_retries}: {infomax_code}")
                
                # ì‹œë„ë§ˆë‹¤ íƒ€ì„ì•„ì›ƒ ì¦ê°€
                timeout = base_timeout + (attempt * 5000)  # 10ì´ˆ, 15ì´ˆ, 20ì´ˆ
                
                async with async_playwright() as p:
                    browser = await p.chromium.launch(headless=True)
                    context = await browser.new_context(
                        viewport={'width': 1920, 'height': 1080},
                        user_agent=self.ua.random
                    )
                    # í˜ì´ì§€ íƒ€ì„ì•„ì›ƒ ì„¤ì •
                    context.set_default_timeout(timeout)
                    
                    page = await context.new_page()
                    
                    # ì›¹ì‚¬ì´íŠ¸ í˜ì´ì§€ë¡œ ì´ë™
                    base_url = os.getenv('WEBSITE_BASE_URL', 'https://www.tradingview.com')
                    url = f"{base_url}/symbols/{ticker}/"
                    print(f"ğŸ” ì›¹ì‚¬ì´íŠ¸ URL: {url} (íƒ€ì„ì•„ì›ƒ: {timeout}ms)")
                    await page.goto(url, timeout=timeout)
                    
                    # ë¡œê³  ì´ë¯¸ì§€ ì„ íƒì (ì—¬ëŸ¬ ê°€ëŠ¥ì„± ì‹œë„)
                    selectors = [
                        'img[data-testid="logo"]',
                        '.tv-symbol-header__logo img',
                        '.tv-symbol-header__logo svg',
                        'img[alt*="logo" i]',
                        'img[src*="logo" i]',
                        '.tv-symbol-header img',
                        'header img'
                    ]
                    
                    for selector in selectors:
                        try:
                            element = await page.wait_for_selector(selector, timeout=timeout + 5000)
                            if element:
                                # SVGì¸ ê²½ìš°
                                if 'svg' in selector:
                                    svg_content = await element.inner_html()
                                    await browser.close()
                                    return svg_content.encode('utf-8')
                                # IMGì¸ ê²½ìš°
                                else:
                                    src = await element.get_attribute('src')
                                    if src and src.startswith('http'):
                                        timeout_http = aiohttp.ClientTimeout(total=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                                        async with aiohttp.ClientSession(timeout=timeout_http) as session:
                                            async with session.get(src) as response:
                                                if response.status == 200:
                                                    await browser.close()
                                                    return await response.read()
                        except:
                            continue
                    
                    await browser.close()
                    if attempt < max_retries - 1:
                        print(f"ğŸ”„ ì¬ì‹œë„ ì˜ˆì •: {infomax_code}")
                        continue
                    return None
                    
            except Exception as e:
                print(f"ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì˜¤ë¥˜ ({infomax_code}, ì‹œë„ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    print(f"ğŸ”„ ì¬ì‹œë„ ì˜ˆì •: {infomax_code}")
                    continue
                return None
        
        return None
    
    async def crawl_logo_dev(self, infomax_code: str, api_domain: str) -> Optional[bytes]:
        """logo.dev APIì—ì„œ ë¡œê³  í¬ë¡¤ë§"""
        try:
            if not self.logo_dev_token:
                print("LOGO_DEV_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return None
            
            # API ì¿¼í„° í™•ì¸
            if not await self._check_quota('logo_dev'):
                print("logo.dev ì¼ì¼ ì¿¼í„° ì´ˆê³¼")
                return None
            
            url = f"https://img.logo.dev/{api_domain}?token={self.logo_dev_token}&format=png&size=300&fallback=404"
            
            timeout = aiohttp.ClientTimeout(total=15)  # 15ì´ˆ íƒ€ì„ì•„ì›ƒ
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        # ì¿¼í„° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
                        await self._update_quota('logo_dev')
                        return await response.read()
                    else:
                        print(f"logo.dev API ì˜¤ë¥˜: {response.status}")
                        return None
                        
        except Exception as e:
            print(f"logo.dev í¬ë¡¤ë§ ì˜¤ë¥˜ ({infomax_code}): {e}")
            return None
    
    async def _check_quota(self, provider: str) -> bool:
        """API ì¿¼í„° í™•ì¸"""
        try:
            # ê¸°ì¡´ APIë¥¼ í†µí•´ ì¿¼í„° í™•ì¸
            timeout = aiohttp.ClientTimeout(total=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
            async with aiohttp.ClientSession(timeout=timeout) as session:
                url = f"{self.existing_api_base}/api/query/raw_data/ext_api_quota"
                params = {
                    "api_name": provider,
                    "quota_date": datetime.now().strftime('%Y-%m-%d'),
                    "limit": 1
                }
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        print(f"ğŸ” ì¿¼í„° í™•ì¸ ì‘ë‹µ: {data}")
                        # í˜ì´ì§€ë„¤ì´ì…˜ëœ ì‘ë‹µ êµ¬ì¡° í™•ì¸
                        if 'data' in data and data['data'] and len(data['data']) > 0:
                            used_count = data['data'][0].get('used_count', 0)
                            max_count = data['data'][0].get('max_count', 5000)
                            print(f"ğŸ” ì¿¼í„° ì‚¬ìš©ëŸ‰: {used_count}/{max_count}")
                            return used_count < max_count
                        elif data and len(data) > 0:
                            used_count = data[0].get('used_count', 0)
                            max_count = data[0].get('max_count', 5000)
                            print(f"ğŸ” ì¿¼í„° ì‚¬ìš©ëŸ‰ (ì§ì ‘): {used_count}/{max_count}")
                            return used_count < max_count
            print(f"ğŸ” ì¿¼í„° í™•ì¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ True ë°˜í™˜")
            return True
        except:
            return True
    
    async def _update_quota(self, provider: str):
        """API ì¿¼í„° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸"""
        try:
            # ê¸°ì¡´ APIë¥¼ í†µí•´ ì¿¼í„° ì—…ë°ì´íŠ¸
            timeout = aiohttp.ClientTimeout(total=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
            async with aiohttp.ClientSession(timeout=timeout) as session:
                url = f"{self.existing_api_base}/api/schemas/raw_data/tables/ext_api_quota/upsert"
                data = {
                    "api_name": provider,
                    "quota_date": datetime.now().strftime('%Y-%m-%d'),
                    "used_count": 1,
                    "max_count": 5000
                }
                async with session.post(url, json=data) as response:
                    pass  # ê²°ê³¼ëŠ” ë¬´ì‹œ
        except:
            pass
    
    def convert_image(self, image_data: bytes, infomax_code: str) -> Dict[str, bytes]:
        """ì´ë¯¸ì§€ë¥¼ ë‹¤ì–‘í•œ í¬ê¸°ë¡œ ë³€í™˜ (SVG â†’ PNG/WebP í¬í•¨)"""
        try:
            results = {}
            
            # SVGì¸ ê²½ìš° PNG/WebPë¡œ ë³€í™˜
            if image_data.startswith(b'<svg') or image_data.startswith(b'<?xml'):
                logger.info(f"SVG íŒŒì¼ ê°ì§€: {infomax_code}")
                
                # SVGë¥¼ PIL Imageë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ cairosvg ì‚¬ìš© ì‹œë„
                try:
                    import cairosvg
                    # SVGë¥¼ PNGë¡œ ë³€í™˜
                    png_data = cairosvg.svg2png(bytestring=image_data)
                    image = Image.open(BytesIO(png_data))
                    logger.info(f"SVG â†’ PNG ë³€í™˜ ì„±ê³µ: {infomax_code}")
                except ImportError:
                    logger.warning("cairosvgê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. SVG ì›ë³¸ë§Œ ì €ì¥")
                    return {"original": image_data}
                except Exception as e:
                    logger.error(f"SVG ë³€í™˜ ì‹¤íŒ¨: {e}")
                    return {"original": image_data}
            else:
                # ì¼ë°˜ ì´ë¯¸ì§€ íŒŒì¼
                image_buffer = BytesIO(image_data)
                image = Image.open(image_buffer)
            
            # í‘œì¤€ ì‚¬ì´ì¦ˆë¡œ ë³€í™˜ (í™˜ê²½ë³€ìˆ˜ IMAGE_SIZES ì‚¬ìš©, ê¸°ë³¸ 240,300)
            sizes_env = os.getenv('IMAGE_SIZES', '240,300')
            try:
                sizes = [int(s.strip()) for s in sizes_env.split(',') if s.strip()]
            except Exception:
                sizes = [240, 300]
            
            formats = ['PNG', 'WebP']
            
            # ì›ë³¸ë„ ì €ì¥ (SVGì¸ ê²½ìš°)
            if image_data.startswith(b'<svg') or image_data.startswith(b'<?xml'):
                results["original"] = image_data
            
            for size in sizes:
                for format_type in formats:
                    try:
                        # ë¦¬ì‚¬ì´ì¦ˆ
                        resized = image.resize((size, size), Image.Resampling.LANCZOS)
                        
                        # ë°”ì´íŠ¸ë¡œ ë³€í™˜
                        output = BytesIO()
                        if format_type == 'PNG':
                            resized.save(output, format='PNG', optimize=True)
                        else:  # WebP
                            resized.save(output, format='WebP', quality=85, optimize=True)
                        
                        results[f"{format_type.lower()}_{size}"] = output.getvalue()
                        logger.debug(f"ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {format_type.lower()}_{size}px")
                        
                    except Exception as e:
                        logger.error(f"ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨ ({format_type}_{size}px): {e}")
                        continue
            
            logger.info(f"ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {infomax_code}, {len(results)}ê°œ íŒŒì¼")
            return results
            
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ë³€í™˜ ì˜¤ë¥˜ ({infomax_code}): {e}")
            # ë³€í™˜ ì‹¤íŒ¨í•´ë„ ì›ë³¸ì€ ì €ì¥
            result = {"original": image_data}
            logger.info(f"ë³€í™˜ ì‹¤íŒ¨ë¡œ ì›ë³¸ë§Œ ë°˜í™˜: {len(result)}ê°œ")
            return result
    
    async def save_to_minio(self, image_data: bytes, object_key: str, content_type: str = "image/png"):
        """MinIOì— ì´ë¯¸ì§€ ì €ì¥"""
        try:
            self.minio_client.put_object(
                self.bucket,
                object_key,
                BytesIO(image_data),
                len(image_data),
                content_type=content_type
            )
            return True
        except Exception as e:
            print(f"MinIO ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    async def save_to_database(self, infomax_code: str, logo_hash: str, file_info: Dict):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œê³  ì •ë³´ ì €ì¥ (ì§ì ‘ API í˜¸ì¶œ)"""
        print(f"ğŸ” DB ì €ì¥ ì‹œì‘: {infomax_code}, {logo_hash}")
        print(f"ğŸ” file_info: {file_info}")
        try:
            # ì§ì ‘ ê¸°ì¡´ API í˜¸ì¶œ
            timeout = aiohttp.ClientTimeout(total=15)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                
                # 1. logos í…Œì´ë¸” í™•ì¸/ìƒì„±
                logo_url = f"{self.existing_api_base}/api/schemas/raw_data/tables/logos/query"
                logo_params = {
                    "search_column": "logo_hash",
                    "search": logo_hash,
                    "limit": 1
                }
                
                async with session.get(logo_url, params=logo_params) as response:
                    if response.status == 200:
                        logo_data = await response.json()
                        if logo_data and 'data' in logo_data and logo_data['data']:
                            logo_id = logo_data['data'][0]['logo_id']
                            print(f"âœ… ê¸°ì¡´ logos ë°ì´í„° ì‚¬ìš©: logo_id={logo_id}")
                        else:
                            # ìƒˆë¡œ ìƒì„±
                            logo_upsert_url = f"{self.existing_api_base}/api/schemas/raw_data/tables/logos/upsert"
                            logo_upsert_data = {
                                "data": {
                                    "logo_hash": logo_hash,
                                    "is_deleted": False
                                },
                                "conflict_columns": ["logo_hash"]
                            }
                            
                            async with session.post(logo_upsert_url, json=logo_upsert_data) as upsert_response:
                                if upsert_response.status == 200:
                                    upsert_data = await upsert_response.json()
                                    logo_id = upsert_data['data']['logo_id']
                                    print(f"âœ… logos í…Œì´ë¸” ì €ì¥ ì„±ê³µ: logo_id={logo_id}")
                                else:
                                    print(f"âŒ logos í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨: {upsert_response.status}")
                                    return False
                    else:
                        print(f"âŒ logos í…Œì´ë¸” ì¡°íšŒ ì‹¤íŒ¨: {response.status}")
                        return False
                
                # 2. logo_files í…Œì´ë¸” ì €ì¥
                file_url = f"{self.existing_api_base}/api/schemas/raw_data/tables/logo_files/upsert"
                file_data = {
                    "data": {
                        "logo_id": logo_id,
                        "file_format": file_info['format'],
                        "dimension_width": file_info['dimension_width'],
                        "dimension_height": file_info['dimension_height'],
                        "file_size": file_info['file_size'],
                        "minio_object_key": file_info['object_key'],
                        "data_source": file_info['data_source'],
                        "upload_type": "crawled",
                        "is_original": file_info.get('is_original', True)
                    },
                    "conflict_columns": ["minio_object_key"]
                }
                
                print(f"ğŸ” logo_files ì €ì¥ ë°ì´í„°: {file_data}")
                
                async with session.post(file_url, json=file_data) as file_response:
                    if file_response.status == 200:
                        print(f"âœ… logo_files í…Œì´ë¸” ì €ì¥ ì„±ê³µ: logo_id={logo_id}")
                        print(f"âœ… DB ì €ì¥ ì™„ë£Œ: {infomax_code}")
                        return True
                    else:
                        error_text = await file_response.text()
                        print(f"âŒ logo_files í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨: {file_response.status}")
                        print(f"âŒ ì˜¤ë¥˜ ë‚´ìš©: {error_text}")
                        return False
                        
        except Exception as e:
            print(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    async def crawl_logo(self, infomax_code: str, ticker: str, api_domain: str = None) -> bool:
        """ë¡œê³  í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸ”ğŸ”ğŸ” CRAWL_LOGO í•¨ìˆ˜ ì§„ì…: {infomax_code}")
        print(f"ğŸ”ğŸ”ğŸ” íŒŒë¼ë¯¸í„°: ticker={ticker}, api_domain={api_domain}")
        try:
            print(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {infomax_code}, ticker={ticker}, api_domain={api_domain}")
            print(f"ğŸ” í•¨ìˆ˜ ì§„ì… í™•ì¸: {infomax_code}")
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì • (30ì´ˆ)
            import asyncio
            try:
                result = await asyncio.wait_for(
                    self._crawl_logo_internal(infomax_code, ticker, api_domain),
                    timeout=30.0
                )
                print(f"ğŸ”ğŸ”ğŸ” CRAWL_LOGO í•¨ìˆ˜ ì™„ë£Œ: {infomax_code}, ê²°ê³¼: {result}")
                return result
            except asyncio.TimeoutError:
                print(f"ğŸ” í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ: {infomax_code}")
                return False
                
        except Exception as e:
            print(f"ğŸ” í¬ë¡¤ë§ í•¨ìˆ˜ ì˜¤ë¥˜: {infomax_code} - {e}")
            import traceback
            print(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return False
    
    async def _crawl_logo_internal(self, infomax_code: str, ticker: str, api_domain: str = None) -> bool:
        """ë¡œê³  í¬ë¡¤ë§ ë‚´ë¶€ í•¨ìˆ˜"""
        try:
            print(f"ğŸ” ë‚´ë¶€ í•¨ìˆ˜ ì§„ì…: {infomax_code}")
            image_data = None
            data_source = None
            logo_hash = None
            
            # ì›¹ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ ì‹œë„ (tickerê°€ ìˆì„ ë•Œë§Œ)
            if ticker and ticker.strip():
                print(f"ğŸ” ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œë„: {infomax_code}")
                try:
                    image_data = await self.crawl_website(infomax_code, ticker)
                    if image_data:
                        data_source = "website"
                        logo_hash = hashlib.md5(f"website_{infomax_code}".encode()).hexdigest()
                        print(f"ğŸ” ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì„±ê³µ: {infomax_code}")
                    else:
                        print(f"ğŸ” ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {infomax_code}")
                except Exception as e:
                    print(f"ğŸ” ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {infomax_code} - {e}")
            
            # ì›¹ì‚¬ì´íŠ¸ ì‹¤íŒ¨ ì‹œ logo.dev ì‹œë„
            if not image_data and api_domain:
                print(f"ğŸ” logo.dev í¬ë¡¤ë§ ì‹œë„: {infomax_code}")
                try:
                    image_data = await self.crawl_logo_dev(infomax_code, api_domain)
                    if image_data:
                        data_source = "logo_dev"
                        logo_hash = hashlib.md5(f"logo_dev_{infomax_code}".encode()).hexdigest()
                        print(f"ğŸ” logo.dev í¬ë¡¤ë§ ì„±ê³µ: {infomax_code}")
                    else:
                        print(f"ğŸ” logo.dev í¬ë¡¤ë§ ì‹¤íŒ¨: {infomax_code}")
                except Exception as e:
                    print(f"ğŸ” logo.dev í¬ë¡¤ë§ ì˜¤ë¥˜: {infomax_code} - {e}")
            
            if not image_data:
                print(f"âŒ ëª¨ë“  í¬ë¡¤ë§ ì‹œë„ ì‹¤íŒ¨: {infomax_code}")
                return False
            
            # ì´ë¯¸ì§€ ë³€í™˜
            print(f"ğŸ” ì´ë¯¸ì§€ ë³€í™˜ ì‹œì‘: {infomax_code}")
            converted_images = self.convert_image(image_data, infomax_code)
            print(f"ğŸ” ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {infomax_code}")
            
            # masterì—ì„œ logo_hash ì¡°íšŒ
            print(f"ğŸ” masterì—ì„œ logo_hash ì¡°íšŒ: {infomax_code}")
            try:
                master_result = self.existing_api.query_table("raw_data", "logo_master", {
                    "search_column": "infomax_code",
                    "search": infomax_code,
                    "limit": 1
                })
                
                if master_result and 'data' in master_result and master_result['data']:
                    logo_hash = master_result['data'][0]['logo_hash']
                    print(f"ğŸ” master logo_hash ì¡°íšŒ ì„±ê³µ: {logo_hash}")
                else:
                    print(f"âŒ master logo_hash ì¡°íšŒ ì‹¤íŒ¨: {infomax_code}")
                    return False
            except Exception as e:
                print(f"âŒ master ì¡°íšŒ ì˜¤ë¥˜: {e}")
                return False
            
            # MinIOì— ì €ì¥
            saved_files = []
            print(f"ğŸ” ë³€í™˜ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(converted_images)}")
            for format_key, img_data in converted_images.items():
                if format_key == "original":
                    # masterì˜ logo_hash ì‚¬ìš©
                    object_key = f"{logo_hash}_original.svg"
                    content_type = "image/svg+xml"
                    is_original = True
                    format_type = "svg"
                    size = None
                else:
                    format_type, size = format_key.split('_')
                    # masterì˜ logo_hash ì‚¬ìš©
                    object_key = f"{logo_hash}_{size}.{format_type.lower()}"
                    content_type = f"image/{format_type.lower()}"
                    is_original = False
                
                print(f"ğŸ” MinIO ì €ì¥ ì‹œë„: {object_key}")
                if await self.save_to_minio(img_data, object_key, content_type):
                    print(f"âœ… MinIO ì €ì¥ ì„±ê³µ: {object_key}")
                    saved_files.append({
                        'object_key': object_key,
                        'format': format_type.lower(),
                        'dimension_width': int(size) if size else None,
                        'dimension_height': int(size) if size else None,
                        'file_size': len(img_data),
                        'is_original': is_original,
                        'data_source': data_source
                    })
                else:
                    print(f"âŒ MinIO ì €ì¥ ì‹¤íŒ¨: {object_key}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            print(f"ğŸ” saved_files ê°œìˆ˜: {len(saved_files)}")
            if not saved_files:
                print(f"âŒ ì €ì¥í•  íŒŒì¼ì´ ì—†ìŒ: {infomax_code}")
                return False
                
            for file_info in saved_files:
                print(f"ğŸ” íŒŒì¼ ì €ì¥: {file_info}")
                # DB ì €ì¥ì€ API ì„œë²„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
                print(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ (DBëŠ” ë³„ë„ ì²˜ë¦¬): {infomax_code}")
                # DB ì €ì¥ ë¹„í™œì„±í™” - API ì„œë²„ì—ì„œ ì²˜ë¦¬
                # save_to_database í•¨ìˆ˜ í˜¸ì¶œ ì™„ì „ ì œê±°
            
            print(f"ë¡œê³  í¬ë¡¤ë§ ì„±ê³µ: {infomax_code} ({len(saved_files)}ê°œ íŒŒì¼)")
            return True
            
        except Exception as e:
            print(f"ë¡œê³  í¬ë¡¤ë§ ì˜¤ë¥˜ ({infomax_code}): {e}")
            return False
    
    async def crawl_batch(self, tickers: List[Dict], job_id: str = None) -> str:
        """ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰"""
        if not job_id:
            job_id = f"crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ì§„í–‰ìƒí™© íŒŒì¼ ìƒì„±
        progress_file = Path("progress") / f"{job_id}.json"
        progress_file.parent.mkdir(exist_ok=True)
        
        progress_data = {
            "job_id": job_id,
            "status": "running",
            "total": len(tickers),
            "completed": 0,
            "success": 0,
            "failed": 0,
            "current": "",
            "started_at": datetime.now().isoformat(),
            "errors": []
        }
        
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        
        try:
            for i, ticker_info in enumerate(tickers):
                infomax_code = ticker_info['infomax_code']
                ticker = ticker_info['ticker']
                api_domain = ticker_info.get('api_domain')
                
                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                progress_data['current'] = f"{infomax_code} ({ticker})"
                progress_data['completed'] = i
                
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, ensure_ascii=False, indent=2)
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                success = await self.crawl_logo(infomax_code, ticker, api_domain)
                
                if success:
                    progress_data['success'] += 1
                else:
                    progress_data['failed'] += 1
                    progress_data['errors'].append(f"Failed: {infomax_code}")
            
            # ì™„ë£Œ ì²˜ë¦¬
            progress_data['status'] = "completed"
            progress_data['completed'] = len(tickers)
            progress_data['finished_at'] = datetime.now().isoformat()
            
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
            
            return job_id
            
        except Exception as e:
            progress_data['status'] = "error"
            progress_data['error'] = str(e)
            progress_data['finished_at'] = datetime.now().isoformat()
            
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
            
            raise e
